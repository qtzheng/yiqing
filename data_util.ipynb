{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self,category,query1,query2,label):\n",
    "        self.re_punctuation='[{}]+'.format(''';'\",.!?；‘’“”，。！？''')\n",
    "        self.category=category\n",
    "        self.query1=re.sub(self.re_punctuation, '', query1)\n",
    "        self.query2=re.sub(self.re_punctuation, '', query2 )\n",
    "        self.label=int(label)\n",
    "        \n",
    "    def convert_to_features(self,tokenizer,trans=False):\n",
    "        encode_data=None\n",
    "        if trans:\n",
    "            encode_data=tokenizer.encode_plus(self.query2,self.query1,max_length=64,pad_to_max_length=True)\n",
    "        else:\n",
    "            encode_data=tokenizer.encode_plus(self.query1,self.query2,max_length=64,pad_to_max_length=True)\n",
    "        return InputFeatures(encode_data['input_ids'],encode_data['token_type_ids'],encode_data['attention_mask'],self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess(object):\n",
    "    def __init__(self,data_path):\n",
    "        self.data_path=data_path\n",
    "        self.tokenizer=tokenizer\n",
    "        \n",
    "    def getTrainDataSet(self,file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = 'train.csv'\n",
    "        examples = self._get_examples(os.path.join(self.data_path,file_name))\n",
    "        features = self._get_features(examples,is_exchange=False)\n",
    "        return self._get_dataset(features),len(features)\n",
    "    \n",
    "    def getValidDataSet(self,file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = 'dev.csv'\n",
    "        examples = self._get_examples(os.path.join(self.data_path,file_name))\n",
    "        features = self._get_features(examples,is_exchange=False)\n",
    "        return self._get_dataset(features),len(features)\n",
    "    \n",
    "    def getTestDataSet(self,file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = 'test.csv'\n",
    "        examples = self._get_examples(os.path.join(self.data_path,file_name))\n",
    "        features = self._get_features(examples,is_exchange=False)\n",
    "        return self._get_dataset(features),len(features)\n",
    "    \n",
    "    def savePredictData(self,file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = 'result.csv'\n",
    "    \n",
    "    def _get_examples(self,file_name):\n",
    "        if os.path.exists(file_name):\n",
    "            data = pd.read_csv(file_name).dropna()\n",
    "            examples = []\n",
    "            for i,line in data.iterrows():\n",
    "                examples.append(InputExample(line['category'],line['query1'],line['query2'],line['label']))\n",
    "            return examples   \n",
    "        else:\n",
    "            raise FileNotFoundError('{0} not found.'.format(data_path))   \n",
    "    def _get_features(self,examples,is_exchange=True):\n",
    "        features=[]\n",
    "        for e in examples:\n",
    "            features.append(e.convert_to_features(self.tokenizer,False))\n",
    "            if is_exchange:\n",
    "                features.append(e.convert_to_features(self.tokenizer,True))\n",
    "        return features\n",
    "    \n",
    "    def _get_dataset(self,features):\n",
    "        def gen():\n",
    "            for ex in features:\n",
    "                yield ({'input_ids': ex.input_ids,'attention_mask': ex.attention_mask,'token_type_ids': ex.token_type_ids},ex.label)\n",
    "        return tf.data.Dataset.from_generator(gen,\n",
    "                                              ({'input_ids': tf.int32,\n",
    "                                                'attention_mask': tf.int32,\n",
    "                                                'token_type_ids': tf.int32},\n",
    "                                               tf.int64),\n",
    "                                              ({'input_ids': tf.TensorShape([None]),\n",
    "                                                'attention_mask': tf.TensorShape([None]),\n",
    "                                                'token_type_ids': tf.TensorShape([None])},\n",
    "                                               tf.TensorShape([])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
