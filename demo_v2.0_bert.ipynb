{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import TFBertPreTrainedModel,BertTokenizer,BertConfig,TFBertMainLayer,TFBertForSequenceClassification\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "from tensorflow.python.platform import tf_logging as logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 阿 斯 蒂 芬 [SEP] 萨 法 [SEP]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode([101, 7350, 3172, 5881, 5705, 102, 5855, 3791, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"model/chinese-roberta-wwm-ext-large\")\n",
    "class InputFeatures(object):\n",
    "    def __init__(self,input_ids,token_type_ids,attention_mask,label):\n",
    "        self.input_ids=input_ids\n",
    "        self.token_type_ids=token_type_ids\n",
    "        self.attention_mask=attention_mask \n",
    "        self.label=label\n",
    "#         if label == 0:\n",
    "#             self.label=[1,0]\n",
    "#         else:\n",
    "#             self.label=[0,1]\n",
    "        \n",
    "        \n",
    "class InputExample(object):\n",
    "    def __init__(self,category,query1,query2,label):\n",
    "        self.re_punctuation='[{}]+'.format(''';'\",.!?；‘’“”，。！？''')\n",
    "        self.category=category\n",
    "        self.query1=re.sub(self.re_punctuation, '', query1)\n",
    "        self.query2=re.sub(self.re_punctuation, '', query2 )\n",
    "        self.label=int(label)\n",
    "        \n",
    "    def convert_to_features(self,tokenizer,trans=False):\n",
    "        encode_data=None\n",
    "        if trans:\n",
    "            encode_data=tokenizer.encode_plus(self.query2,self.query1,max_length=64,pad_to_max_length=True)\n",
    "        else:\n",
    "            encode_data=tokenizer.encode_plus(self.query1,self.query2,max_length=64,pad_to_max_length=True)\n",
    "        return InputFeatures(encode_data['input_ids'],encode_data['token_type_ids'],encode_data['attention_mask'],self.label)\n",
    "\n",
    "class DataProcess(object):\n",
    "    def __init__(self,data_path,tokenizer):\n",
    "        self.data_path=data_path\n",
    "        self.tokenizer=tokenizer\n",
    "        \n",
    "    def getTrainDataSet(self,file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = 'train.csv'\n",
    "        examples = self._get_examples(os.path.join(self.data_path,file_name))\n",
    "        features = self._get_features(examples,is_exchange=False)\n",
    "        return self._get_dataset(features),len(features)\n",
    "    \n",
    "    def getValidDataSet(self,file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = 'dev.csv'\n",
    "        examples = self._get_examples(os.path.join(self.data_path,file_name))\n",
    "        features = self._get_features(examples,is_exchange=False)\n",
    "        return self._get_dataset(features),len(features)\n",
    "    \n",
    "    def getTestDataSet(self,file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = 'test.csv'\n",
    "        examples = self._get_examples(os.path.join(self.data_path,file_name))\n",
    "        features = self._get_features(examples,is_exchange=False)\n",
    "        return self._get_dataset(features),len(features)\n",
    "    \n",
    "    def savePredictData(self,file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = 'result.csv'\n",
    "    \n",
    "    def _get_examples(self,file_name):\n",
    "        if os.path.exists(file_name):\n",
    "            data = pd.read_csv(file_name).dropna()\n",
    "            examples = []\n",
    "            for i,line in data.iterrows():\n",
    "                examples.append(InputExample(line['category'],line['query1'],line['query2'],line['label']))\n",
    "            return examples   \n",
    "        else:\n",
    "            raise FileNotFoundError('{0} not found.'.format(data_path))   \n",
    "    def _get_features(self,examples,is_exchange=True):\n",
    "        features=[]\n",
    "        for e in examples:\n",
    "            features.append(e.convert_to_features(self.tokenizer,False))\n",
    "            if is_exchange:\n",
    "                features.append(e.convert_to_features(self.tokenizer,True))\n",
    "        return features\n",
    "    \n",
    "    def _get_dataset(self,features):\n",
    "        def gen():\n",
    "            for ex in features:\n",
    "                yield ({'input_ids': ex.input_ids,'attention_mask': ex.attention_mask,'token_type_ids': ex.token_type_ids},ex.label)\n",
    "        return tf.data.Dataset.from_generator(gen,\n",
    "                                              ({'input_ids': tf.int32,\n",
    "                                                'attention_mask': tf.int32,\n",
    "                                                'token_type_ids': tf.int32},\n",
    "                                               tf.int32),\n",
    "                                              ({'input_ids': tf.TensorShape([None]),\n",
    "                                                'attention_mask': tf.TensorShape([None]),\n",
    "                                                'token_type_ids': tf.TensorShape([None])},\n",
    "                                               tf.TensorShape([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_process = DataProcess(data_path='data',tokenizer=bert_tokenizer)\n",
    "train_dataset,train_length = data_process.getTrainDataSet()\n",
    "valid_dataset,valid_length = data_process.getValidDataSet()\n",
    "tests_dataset,tests_length = data_process.getTestDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(train_length).batch(64).repeat(-1)\n",
    "valid_dataset = valid_dataset.shuffle(valid_length).batch(64).repeat(-1)\n",
    "tests_dataset = tests_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForYiQing(TFBertPreTrainedModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.bert = TFBertMainLayer(config, name=\"bert\")\n",
    "        self.bert.pool = tf.keras.layers.LSTM(config.hidden_size,kernel_initializer=get_initializer(config.initializer_range))\n",
    "        self.num_labels = config.num_labels\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            1, kernel_initializer=get_initializer(config.initializer_range), activation='sigmoid',name=\"classifier\"\n",
    "        )\n",
    "        \n",
    "#         self.lstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences= True,dropout=0.2))\n",
    "#         self.lstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,return_sequences= True,dropout=0.2))\n",
    "#         self.lstm3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,return_sequences= True,dropout=0.2))\n",
    "#         self.lstm4 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,dropout=0.2))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=kwargs.get(\"training\", False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "model = TFBertForYiQing.from_pretrained('model/chinese-roberta-wwm-ext-large')\n",
    "# model = TFBertForSequenceClassification.from_pretrained('model/chinese-roberta-wwm-ext-large')\n",
    "model.bert.trainable=True\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(2e-5),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_yi_qing_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  325522432 \n",
      "_________________________________________________________________\n",
      "dropout_369 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1025      \n",
      "=================================================================\n",
      "Total params: 325,523,457\n",
      "Trainable params: 325,523,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,monitor='val_accuracy',baseline=0.85,target_accuracy = 0.877):\n",
    "        self.baseline=baseline\n",
    "        self.monitor=monitor\n",
    "        self.target_accuracy=target_accuracy\n",
    "        self.count = 0\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = self.get_monitor_value(logs)\n",
    "        if current > self.baseline and not self.model.transformer.trainable:\n",
    "            logging.info('current `%s` is %s ,begin train all params',self.monitor,current)\n",
    "            self.model.bert.trainable=True\n",
    "            self.model.summary()\n",
    "        if current > self.target_accuracy and self.count > 2:\n",
    "            self.model.stop_training = True\n",
    "        else:\n",
    "            self.count = self.count + 1\n",
    "        \n",
    "    def get_monitor_value(self, logs):\n",
    "        logs = logs or {}\n",
    "        monitor_value = logs.get(self.monitor)\n",
    "        if monitor_value is None:\n",
    "            monitor_value = 0.0\n",
    "        return monitor_value\n",
    "\n",
    "custom_callback = CustomCallback(baseline=0.86,target_accuracy=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 137 steps, validate for 32 steps\n",
      "Epoch 1/3\n",
      "137/137 - 106s - loss: 0.6751 - accuracy: 0.5957 - val_loss: 0.6732 - val_accuracy: 0.5984\n",
      "Epoch 2/3\n",
      "137/137 - 78s - loss: 0.6745 - accuracy: 0.5991 - val_loss: 0.6716 - val_accuracy: 0.5984\n",
      "Epoch 3/3\n",
      "137/137 - 78s - loss: 0.6735 - accuracy: 0.6007 - val_loss: 0.6720 - val_accuracy: 0.5984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x207549eaa08>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps = train_length//64+1\n",
    "valid_steps = valid_length//64+1\n",
    "model.fit(train_dataset,\n",
    "          epochs=3,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=valid_steps,\n",
    "          verbose=2,\n",
    "#           callbacks=[custom_callback]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset,test_length=data_process.getTestDataSet()\n",
    "test_dataset = test_dataset.batch(2)\n",
    "test_steps = test_length//2 +1\n",
    "predict_data = model.predict(test_dataset,steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = np.squeeze(predict_data)+0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = [ [i,d] for i,d in enumerate(predict_data.astype(int))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = pd.DataFrame(predict_data,columns=['id','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   0      1\n",
       "1   1      1\n",
       "2   2      1\n",
       "3   3      0\n",
       "4   4      0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
