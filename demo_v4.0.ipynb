{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import XLNetTokenizer, TFXLNetModel,TFXLNetPreTrainedModel,TFXLNetMainLayer,TFSequenceSummary,XLNetConfig,XLNetModel\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "from tensorflow.python.platform import tf_logging as logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlnet_config = XLNetConfig.from_json_file('model/xlnet_mid/config.json')\n",
    "# xlnet_model=TFXLNetModel.from_pretrained('model/xlnet_mid/pytorch_model.bin',config=xlnet_config,from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids, token_type_ids, attention_mask, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.label = int(label)\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    def __init__(self, category, query1, query2, label):\n",
    "        self.re_punctuation = '[{}]+'.format(''';'\",.!?；‘’“”，。！？''')\n",
    "        self.category = category\n",
    "        self.query1 = re.sub(self.re_punctuation, '', query1)\n",
    "        self.query2 = re.sub(self.re_punctuation, '', query2)\n",
    "        self.label = int(label)\n",
    "\n",
    "    def convert_to_features(self, tokenizer, trans=False):\n",
    "        encode_data = None\n",
    "        if trans:\n",
    "            encode_data = tokenizer.encode_plus(self.query2, self.query1, max_length=64, pad_to_max_length=True)\n",
    "        else:\n",
    "            encode_data = tokenizer.encode_plus(self.query1, self.query2, max_length=64, pad_to_max_length=True)\n",
    "        return InputFeatures(encode_data['input_ids'], encode_data['token_type_ids'], encode_data['attention_mask'],\n",
    "                             self.label)\n",
    "\n",
    "\n",
    "class DataProcess(object):\n",
    "    def __init__(self, data_path, tokenizer, model):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def getDataSet(self, file_name):\n",
    "        examples = self._get_examples(os.path.join(self.data_path, file_name))\n",
    "        features, labels = self._get_features(examples)\n",
    "        length = len(features)\n",
    "        features = tf.data.Dataset.from_tensor_slices(features)\n",
    "        features = tf.data.Dataset.zip((features, labels))\n",
    "        return features, length\n",
    "\n",
    "    def savePredictData(self, file_name='result.csv'):\n",
    "        if file_name is None:\n",
    "            file_name = 'result.csv'\n",
    "\n",
    "    def _get_examples(self, file_name):\n",
    "        if os.path.exists(file_name):\n",
    "            data = pd.read_csv(file_name).dropna()\n",
    "            examples = []\n",
    "            for i, line in data.iterrows():\n",
    "                examples.append(InputExample(line['category'], line['query1'], line['query2'], line['label']))\n",
    "            return examples\n",
    "        else:\n",
    "            raise FileNotFoundError('{0} not found.'.format(file_name))\n",
    "\n",
    "    def _get_features(self, examples):\n",
    "        features_a = []\n",
    "        features_b = []\n",
    "        labels = []\n",
    "        for e in examples:\n",
    "            eccode_a = self.tokenizer.encode_plus(e.query1, max_length=32, pad_to_max_length=True)\n",
    "            eccode_b = self.tokenizer.encode_plus(e.query2, max_length=32, pad_to_max_length=True)\n",
    "\n",
    "            features_a.append(\n",
    "                InputFeatures(eccode_a['input_ids'], eccode_a['token_type_ids'], eccode_a['attention_mask'], e.label))\n",
    "            features_b.append(\n",
    "                InputFeatures(eccode_b['input_ids'], eccode_b['token_type_ids'], eccode_b['attention_mask'], e.label))\n",
    "            labels.append([e.label])\n",
    "        features_a = self._get_dataset(features_a).batch(64)\n",
    "        features_b = self._get_dataset(features_b).batch(64)\n",
    "\n",
    "        steps = len(labels) // 64 + 1\n",
    "        encode_a = self.model.predict(features_a, steps=steps)\n",
    "        encode_b = self.model.predict(features_b, steps=steps)\n",
    "        assert len(encode_a) == len(labels)\n",
    "        assert len(encode_b) == len(labels)\n",
    "\n",
    "        return tf.concat([encode_a, encode_b], axis=1), tf.data.Dataset.from_tensor_slices(labels)\n",
    "\n",
    "    def _get_dataset(self, features):\n",
    "        def gen():\n",
    "            for ex in features:\n",
    "                yield {'input_ids': ex.input_ids, 'attention_mask': ex.attention_mask,'token_type_ids': ex.token_type_ids}\n",
    "\n",
    "        return tf.data.Dataset.from_generator(gen,\n",
    "                                              {'input_ids': tf.int32,\n",
    "                                                'attention_mask': tf.int32,\n",
    "                                                'token_type_ids': tf.int32},\n",
    "                                              {'input_ids': tf.TensorShape([None]),\n",
    "                                                'attention_mask': tf.TensorShape([None]),\n",
    "                                                'token_type_ids': tf.TensorShape([None])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained('model/xlnet')\n",
    "xlnet_model = TFXLNetModel.from_pretrained('model/xlnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_process = DataProcess(data_path='data', tokenizer=xlnet_tokenizer, model=xlnet_model)\n",
    "train_dataset, train_length = data_process.getDataSet('train.csv')\n",
    "vaild_dataset, valid_length = data_process.getDataSet('dev.csv')\n",
    "tests_dataset, tests_length = data_process.getDataSet('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((64, 768), (1,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(train_length).batch(64).repeat(-1)\n",
    "vaild_dataset = vaild_dataset.shuffle(valid_length).batch(64).repeat(-1)\n",
    "tests_dataset = tests_dataset.shuffle(tests_length).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(512,return_sequences= True,dropout=0.2,input_shape=(None,768)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,return_sequences= True,dropout=0.2)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,return_sequences= True,dropout=0.2)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,dropout=0.2)))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(2e-5),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, None, 512)         2623488   \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, None, 1024)        4198400   \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, None, 1024)        6295552   \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 1024)              6295552   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 19,414,017\n",
      "Trainable params: 19,414,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 136 steps, validate for 31 steps\n",
      "Epoch 1/6\n",
      "136/136 - 18s - loss: 0.4162 - accuracy: 0.8061 - val_loss: 0.6993 - val_accuracy: 0.6552\n",
      "Epoch 2/6\n",
      "136/136 - 16s - loss: 0.3743 - accuracy: 0.8279 - val_loss: 0.7122 - val_accuracy: 0.6764\n",
      "Epoch 3/6\n",
      "136/136 - 16s - loss: 0.3365 - accuracy: 0.8504 - val_loss: 0.7767 - val_accuracy: 0.6809\n",
      "Epoch 4/6\n",
      "136/136 - 16s - loss: 0.2948 - accuracy: 0.8754 - val_loss: 0.8132 - val_accuracy: 0.6895\n",
      "Epoch 5/6\n",
      "136/136 - 16s - loss: 0.2798 - accuracy: 0.8810 - val_loss: 0.7877 - val_accuracy: 0.6739\n",
      "Epoch 6/6\n",
      "136/136 - 16s - loss: 0.2427 - accuracy: 0.8973 - val_loss: 0.8921 - val_accuracy: 0.6850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20b68d75388>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps = train_length//64\n",
    "valid_steps = valid_length//64\n",
    "model.fit(train_dataset,\n",
    "          epochs=6,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_data=vaild_dataset,\n",
    "          validation_steps=valid_steps,\n",
    "          verbose=2,\n",
    "#           callbacks=[custom_callback]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = train_length//64+1\n",
    "valid_steps = vaild_dataset//64+1\n",
    "query_a_datas = model.predict(train_dataset,steps=train_steps)\n",
    "query_b_datas = model.predict(dev_dataset,steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,monitor='val_accuracy',baseline=0.85,target_accuracy = 0.877):\n",
    "        self.baseline=baseline\n",
    "        self.monitor=monitor\n",
    "        self.target_accuracy=target_accuracy\n",
    "        self.count = 0\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = self.get_monitor_value(logs)\n",
    "        if current > self.baseline and not self.model.transformer.trainable:\n",
    "            logging.info('current `%s` is %s ,begin train all params',self.monitor,current)\n",
    "            self.model.transformer.trainable=True\n",
    "            self.model.summary()\n",
    "        if current > self.target_accuracy and self.count > 2:\n",
    "            self.model.stop_training = True\n",
    "        else:\n",
    "            self.count = self.count + 1\n",
    "        \n",
    "    def get_monitor_value(self, logs):\n",
    "        logs = logs or {}\n",
    "        monitor_value = logs.get(self.monitor)\n",
    "        if monitor_value is None:\n",
    "            monitor_value = 0.0\n",
    "        return monitor_value\n",
    "\n",
    "custom_callback = CustomCallback(baseline=0.86,target_accuracy=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 136 steps, validate for 31 steps\n",
      "Epoch 1/6\n",
      "136/136 - 68s - loss: 0.3340 - accuracy: 0.8543 - val_loss: 0.2988 - val_accuracy: 0.8740\n",
      "Epoch 2/6\n",
      "136/136 - 68s - loss: 0.3271 - accuracy: 0.8543 - val_loss: 0.2894 - val_accuracy: 0.8821\n",
      "Epoch 3/6\n",
      "136/136 - 68s - loss: 0.3128 - accuracy: 0.8620 - val_loss: 0.2759 - val_accuracy: 0.8795\n",
      "Epoch 4/6\n",
      "136/136 - 68s - loss: 0.3192 - accuracy: 0.8552 - val_loss: 0.3264 - val_accuracy: 0.8725\n",
      "Epoch 5/6\n",
      "136/136 - 68s - loss: 0.3089 - accuracy: 0.8647 - val_loss: 0.3033 - val_accuracy: 0.8765\n",
      "Epoch 6/6\n",
      "136/136 - 68s - loss: 0.3088 - accuracy: 0.8623 - val_loss: 0.3070 - val_accuracy: 0.8780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14e3cb31a08>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps = train_length//64\n",
    "valid_steps = dev_length//64\n",
    "model.fit(train_dataset,\n",
    "          epochs=6,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_data=dev_dataset,\n",
    "          validation_steps=valid_steps,\n",
    "          verbose=2,\n",
    "#           callbacks=[custom_callback]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset,test_length=data_process.getTestDataSet()\n",
    "test_dataset = test_dataset.batch(2)\n",
    "test_steps = test_length//2 +1\n",
    "predict_data = model.predict(test_dataset,steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = np.squeeze(predict_data)+0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = [ [i,d] for i,d in enumerate(predict_data.astype(int))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = pd.DataFrame(predict_data,columns=['id','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   0      1\n",
       "1   1      1\n",
       "2   2      1\n",
       "3   3      0\n",
       "4   4      0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
